# results_interpretation_by_shap
## Interpretation results of machine learning models by SHAP
The problem of binary classification of breast cancer in women was chosen as an example

### To get an overview of which features are most important for a model we can plot the SHAP values of every feature for every sample. The plot below sorts features by the sum of SHAP value magnitudes over all samples, and uses SHAP values to show the distribution of the impacts each feature has on the model output. 
![Screenshot](summary_plot.jpeg)
